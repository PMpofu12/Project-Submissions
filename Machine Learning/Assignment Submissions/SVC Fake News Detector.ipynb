{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from time import sleep\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from collections import Counter\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jesse Bristow (1875955)\n",
    "#Philani Mpofu (1848751)\n",
    "#Matthew Kruger (1669326)\n",
    "#Chloe Smith (1877342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from .csv file\n",
    "def loadData(name):\n",
    "    return pd.read_csv(name, names = ['ID','Title','Author','Text','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split cleaned data in training, validation, and testing data\n",
    "def getAllData(data):\n",
    "    seventy_percent_of_data = int(data.shape[0]*0.7)\n",
    "    twenty_percent_of_data = int(data.shape[0]*0.2)\n",
    "    \n",
    "    train_data = data[:seventy_percent_of_data]\n",
    "    valid_data = data[seventy_percent_of_data:seventy_percent_of_data+twenty_percent_of_data]\n",
    "    test_data = data[seventy_percent_of_data+twenty_percent_of_data:]\n",
    "    data = None\n",
    "    \n",
    "    #Get author trustworthyness\n",
    "    author_trust_scores = getAuthorTrustworthyness(train_data)\n",
    "\n",
    "    train_data = convertRowsToDataPoints(train_data, testEmbeddingModel, test_model_vocab, 202, author_trust_scores)\n",
    "    valid_data = convertRowsToDataPoints(valid_data, testEmbeddingModel, test_model_vocab, 202, author_trust_scores)\n",
    "    test_data = convertRowsToDataPoints(test_data, testEmbeddingModel, test_model_vocab, 202, author_trust_scores)\n",
    "\n",
    "    train_data_Y = train_data[:,201]\n",
    "    valid_data_Y = valid_data[:,201]\n",
    "    test_data_Y = test_data[:,201]\n",
    "\n",
    "    train_data = np.delete(train_data, 201, 1)\n",
    "    valid_data = np.delete(valid_data, 201, 1)\n",
    "    test_data = np.delete(test_data, 201, 1)\n",
    "    \n",
    "    return train_data, train_data_Y, valid_data, valid_data_Y, test_data, test_data_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and return embedding model used to convert words into vectors\n",
    "def getEmbeddingModel(allData, dimension_for_word):\n",
    "    #Store all titles and text\n",
    "    allTitles = allData[:,1]\n",
    "    allText = allData[:,3]\n",
    "    allTitlesAndText = np.concatenate((allText,allTitles))\n",
    "    allTitles = None\n",
    "    allText = None\n",
    "    \n",
    "    #Store in format(rows are sentences and columns in row are words) used to train the embedding model\n",
    "    dataForEmbedding = []\n",
    "    remove = string.punctuation\n",
    "    \n",
    "    for currTitleOrText in allTitlesAndText:\n",
    "        #For each title or text convert to list sentence and add to data\n",
    "        \n",
    "        for j in sent_tokenize(currTitleOrText):\n",
    "            #For each sentence in the title or text\n",
    "                \n",
    "            temp = []\n",
    "            \n",
    "            for k in word_tokenize(j):\n",
    "                # tokenize the sentence into words           \n",
    "                curr_word = k + ''\n",
    "                #remove all punctuation from word\n",
    "                curr_word = curr_word.translate(str.maketrans('', '', remove))\n",
    "                \n",
    "                #If word is longer than 0 then convert to lowercase and store\n",
    "                if len(curr_word) > 0:\n",
    "                    temp.append(curr_word.lower()) \n",
    "  \n",
    "            if len(temp) > 0:\n",
    "                dataForEmbedding.append(temp) \n",
    "            \n",
    "    \n",
    "    #Train embedding model with sentences.\n",
    "    embeddingModel = gensim.models.Word2Vec(dataForEmbedding, min_count = 1,  size = dimension_for_word, window = 5)\n",
    "    \n",
    "    return embeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getAuthorTrustworthyness(allData):\n",
    "    allAuthors = allData[:,2]\n",
    "    allLabels = allData[:,4]\n",
    "    tmp_author_trust = {};\n",
    "    \n",
    "    #For each author\n",
    "    for i in range(len(allAuthors)):\n",
    "        #input info into author trust to perform trustworthy score calculations afterward\n",
    "        curr_author = allAuthors[i]\n",
    "        curr_label = allLabels[i]\n",
    "        if curr_author not in tmp_author_trust.keys():\n",
    "            tmp_author_trust[curr_author] = [0.0, 0.0]\n",
    "            \n",
    "        if curr_label == 0.0:\n",
    "            tmp_author_trust[curr_author][0]+=1.0\n",
    "        elif curr_label == 1.0:\n",
    "            tmp_author_trust[curr_author][1]+=1.0\n",
    "    \n",
    "    author_trust = {};\n",
    "    #For each author in dictionary\n",
    "    for curr_author in tmp_author_trust.keys():\n",
    "        #calculate trustworthy score of author\n",
    "        curr_score = (tmp_author_trust[curr_author][0])/(tmp_author_trust[curr_author][0]+tmp_author_trust[curr_author][1])\n",
    "        author_trust[curr_author] = curr_score/1000.0\n",
    "    \n",
    "    return author_trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean data so that it can be used to train the embedding model/SVM\n",
    "def getCleanedData(data):\n",
    "    #Remove Nan, remove punctuation, and new lines\n",
    "    feature_names = np.array(['ID','Title','Author','Text'])\n",
    "    \n",
    "    #Convert NaNs\n",
    "    for i in range(3):\n",
    "        string_replacement = \"\"\n",
    "        if i==1:\n",
    "            string_replacement = \"-NO AUTHOR-\"\n",
    "        else:\n",
    "            string_replacement = \"NaN\"\n",
    "\n",
    "        for j in range(len(data)):\n",
    "            if pd.isnull(data[j][i+1]):\n",
    "                data[j][2] = string_replacement\n",
    "        \n",
    "    \n",
    "    #Store punctuation to remove\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\".\",\"“\")\n",
    "    remove = remove.replace(\"!\",\"”\")\n",
    "    remove = remove.replace(\"?\",\"’\")\n",
    "    remove = remove + '‘'\n",
    "    remove = remove + '—'\n",
    "    remove = remove + '–'\n",
    "\n",
    "    #Remove NaNs\n",
    "    data = data[np.all(data != \"NaN\", axis = 1)]\n",
    "    \n",
    "    #Remove punctuation(except '.','?','!') and new lines\n",
    "    for i in range(3):\n",
    "        for j in range(len(data)):\n",
    "            data[j][i+1] = data[j][i+1].replace(\"\\n\",\"\").translate(str.maketrans('', '', remove))\n",
    "    \n",
    "    #Return feature names and the data\n",
    "    return feature_names, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all datapoints into a form that can be used to train the SVM\n",
    "def convertRowsToDataPoints(data, embedding_model, model_vocab, num_expected_columns, author_trust_scores): \n",
    "    data_points_with_labels_list = []\n",
    "    for row in data:\n",
    "        convertedRow = convertRowToDataPoint(row, embedding_model, model_vocab, author_trust_scores)\n",
    "        if len(convertedRow) == num_expected_columns:\n",
    "            data_points_with_labels_list.append(convertedRow)\n",
    "        \n",
    "    data_points_with_labels = np.array(data_points_with_labels_list)\n",
    "    return data_points_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert single datapoint into a form that can be used to train the SVM\n",
    "def convertRowToDataPoint(row, embedding_model, model_vocab, author_trust_scores):\n",
    "    #Take in row without label and convert to data point\n",
    "    #Stores author points, text points then title points then label\n",
    "    curr_title = row[1]\n",
    "    curr_text = row[3]\n",
    "    curr_author = row[4]\n",
    "    curr_author_trust_score = 0.0005\n",
    "    if curr_author in author_trust_scores.keys():\n",
    "        curr_author_trust_score = author_trust_scores[curr_author]\n",
    "    \n",
    "    data_point = np.array([row[4]])\n",
    "    data_point = np.append(np.array([curr_author_trust_score]), data_point)\n",
    "    data_point = np.append(getAverageEmbedding(curr_title, embedding_model, model_vocab), data_point)\n",
    "    data_point = np.append(getAverageEmbedding(curr_text, embedding_model, model_vocab), data_point)\n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get average embedding values for words used in title/article\n",
    "def getAverageEmbedding(curr_string, embedding_model, model_vocab):\n",
    "    #Take in string that has already been cleaned of punctuation and newlines\n",
    "    total_words = 0.0\n",
    "    stop_words = ['not', 'you', 'at', 'from', 'of', 'us', 'in', 'have', 'yes', 'no', 'are', '', 'for', 'but', 'that', 'it', 'this','he','she', 'they','that','a','an', 'who', 'where','there', 'his','her','their', 'i','my','we','our','were', 'the','if','as', 'and','in','on','we','to', 'also','so','is','its']\n",
    "    remove = string.punctuation\n",
    "    curr_embedding = np.array([])\n",
    "    for k in word_tokenize(curr_string):\n",
    "        curr_word = k + ''\n",
    "        #remove all punctuation from word\n",
    "        curr_word = curr_word.translate(str.maketrans('', '', remove))\n",
    "        #convert to lowercase\n",
    "        curr_word = curr_word.lower()\n",
    "        #check if it is a stop word or empty\n",
    "        if curr_word in stop_words:\n",
    "            continue\n",
    "        #check if word is in model_vocab\n",
    "        if curr_word not in model_vocab:\n",
    "            continue\n",
    "        \n",
    "        if len(curr_embedding) == 0:\n",
    "            curr_embedding = embedding_model[curr_word]\n",
    "        else:\n",
    "            curr_embedding = curr_embedding+embedding_model[curr_word]\n",
    "        \n",
    "        total_words = total_words + 1.0\n",
    "    \n",
    "    curr_embedding = curr_embedding/total_words\n",
    "    return curr_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print results of model on data\n",
    "def printAccuracy(trained_model, testing_data, testing_Y):\n",
    "    total_predicted = 0.0\n",
    "    total_correct = 0.0\n",
    "    confusion_matrix = np.array([[0, 0],[0, 0]])\n",
    "    \n",
    "    \n",
    "    for i in range(len(testing_data)):\n",
    "        curr_data_point = testing_data[i]\n",
    "        curr_prediction = trained_model.predict([curr_data_point])\n",
    "        curr_answer = testing_Y[i]\n",
    "        \n",
    "        confusion_matrix[int(curr_prediction)][int(curr_answer)]+=1\n",
    "        if curr_answer == curr_prediction:\n",
    "            total_correct = total_correct+1\n",
    "            \n",
    "        total_predicted = total_predicted+1\n",
    "        \n",
    "    percent_correct = total_correct*100/total_predicted\n",
    "    trained_model.printHyperparameters()\n",
    "    print(\"Accuracy: \", percent_correct)\n",
    "    print(\"Confusion Matrix:\\n\",confusion_matrix, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support vector machine class\n",
    "class Support_Vector_Classifier:\n",
    "    \n",
    "    def __init__(self, regularization_strength, learning_rate, convergence_const, num_iterations_training):\n",
    "        self.regularization_strength = regularization_strength\n",
    "        self.learning_rate = learning_rate\n",
    "        self.convergence_const = convergence_const\n",
    "        self.num_iterations_training = num_iterations_training\n",
    "        self.weights = None\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        \n",
    "        \n",
    "    def train(self,X_inp,Y_inp):\n",
    "        #Initialise datamembers\n",
    "        self.X = np.insert(X_inp, X_inp.shape[1], np.full(X_inp.shape[0],1), axis=1)\n",
    "        self.weights = np.zeros(self.X.shape[1])\n",
    "        self.Y = Y_inp\n",
    "        prev_cost_value = -1000\n",
    "        curr_iteration = 0\n",
    "        has_converged = False\n",
    "        \n",
    "        while (curr_iteration<self.num_iterations_training) and (not has_converged):\n",
    "            #Shuffle data set\n",
    "            self.X, self.Y = shuffle(self.X, self.Y, random_state=0)\n",
    "            #For each row get convergence and update\n",
    "            for i in range(self.X.shape[0]):\n",
    "                curr_Xi = self.X[i]\n",
    "                curr_Yi = self.getYi(self.Y[i])\n",
    "                curr_conv_grad = self.getConvGrad(curr_Xi,curr_Yi)\n",
    "                self.weights = self.weights - (self.learning_rate*curr_conv_grad)\n",
    "                \n",
    "            curr_cost_value = self.getCostFunction()\n",
    "            curr_cost_diff = abs(prev_cost_value-curr_cost_value)\n",
    "            if curr_cost_diff<self.convergence_const:\n",
    "                has_converged = True\n",
    "                \n",
    "            curr_iteration = curr_iteration+1\n",
    "            \n",
    "        \n",
    "    def predict(self,predict_X):\n",
    "        #Add 1 to end col\n",
    "        predict_X = np.append(predict_X,np.array(1))\n",
    "        dot_prod = np.dot(self.weights, predict_X)\n",
    "        if dot_prod < 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def getConvGrad(self,Xi,Yi):\n",
    "        curr_slack = self.getCurrSlack(Xi,Yi)\n",
    "        if curr_slack == 0:\n",
    "            return self.weights\n",
    "        else:\n",
    "            return self.weights-(self.regularization_strength*Yi*Xi)\n",
    "        \n",
    "    def getCostFunction(self):\n",
    "        curr_cost = 0.0\n",
    "        total_rows = self.X.shape[0]\n",
    "        for i in range(self.X.shape[0]):\n",
    "            xi = self.X[i]\n",
    "            yi = self.getYi(self.Y[i])\n",
    "            curr_cost = curr_cost + self.getHalfWLengthSqaured() + self.regularization_strength*self.getCurrSlack(xi,yi)\n",
    "            \n",
    "        curr_cost = curr_cost/total_rows\n",
    "        return curr_cost\n",
    "        \n",
    "    def getYi(self, curr_y_value):\n",
    "        if curr_y_value == 0.0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def getCurrSlack(self, Xi, Yi):\n",
    "        return max(0,1-Yi*(np.dot(self.weights,Xi)))\n",
    "    \n",
    "    def getHalfWLengthSqaured(self):\n",
    "        return (np.dot(self.weights, self.weights)**2)/2.0\n",
    "        \n",
    "    def printHyperparameters(self):\n",
    "        print(\"Current model hyperparameters:\\nRegularization strength: \"+\n",
    "              str(self.regularization_strength)+\"\\nLearning rate: \"+\n",
    "              str(self.learning_rate)+\"\\nTermination constant: \"+\n",
    "              str(self.convergence_const)+\"\\nMax number of iterations for training: \"+\n",
    "              str(self.num_iterations_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Load data\n",
    "name = 'Actual Data.csv'\n",
    "data = loadData(name)\n",
    "\n",
    "#Convert data to numpy array\n",
    "data = data.to_numpy()\n",
    "#Clean data \n",
    "test_feature_names, data = getCleanedData(data) \n",
    "#Train and get embedding model\n",
    "testEmbeddingModel = getEmbeddingModel(data, 100)\n",
    "#Get list of words used inside the embedding model\n",
    "test_model_vocab = testEmbeddingModel.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Split data into training/validation/testing data and set data = None to save memory\n",
    "train_data, train_data_Y, valid_data, valid_data_Y, test_data, test_data_Y = getAllData(data)\n",
    "data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train multiple SVC's on validation data to find suitable hyper-parameters\n",
    "my_SVC0 = Support_Vector_Classifier(10000, 0.0000001, 500, 1000)\n",
    "my_SVC0.train(train_data,train_data_Y)\n",
    "my_SVC1 = Support_Vector_Classifier(1000, 0.0000001, 500, 1000)\n",
    "my_SVC1.train(train_data,train_data_Y)\n",
    "my_SVC2 = Support_Vector_Classifier(100000, 0.0000001, 500, 1000)\n",
    "my_SVC2.train(train_data,train_data_Y)\n",
    "my_SVC3 = Support_Vector_Classifier(10000, 0.000001, 500, 1000)\n",
    "my_SVC3.train(train_data,train_data_Y)\n",
    "my_SVC4 = Support_Vector_Classifier(10000, 0.00000001, 500, 1000)\n",
    "my_SVC4.train(train_data,train_data_Y)\n",
    "my_SVC5 = Support_Vector_Classifier(100000, 0.00000001, 500, 1000)\n",
    "my_SVC5.train(train_data,train_data_Y)\n",
    "my_SVC6 = Support_Vector_Classifier(1000, 0.000001, 500, 1000)\n",
    "my_SVC6.train(train_data,train_data_Y)\n",
    "my_SVC7 = Support_Vector_Classifier(100000, 0.000001, 500, 1000)\n",
    "my_SVC7.train(train_data,train_data_Y)\n",
    "my_SVC8 = Support_Vector_Classifier(1000, 0.00000001, 500, 1000)\n",
    "my_SVC8.train(train_data,train_data_Y)\n",
    "my_SVC9 = Support_Vector_Classifier(50000, 0.00000001, 500, 1000)\n",
    "my_SVC9.train(train_data,train_data_Y)\n",
    "my_SVC10 = Support_Vector_Classifier(5000, 0.000001, 500, 1000)\n",
    "my_SVC10.train(train_data,train_data_Y)\n",
    "my_SVC11 = Support_Vector_Classifier(12500, 0.0000001, 500, 1000)\n",
    "my_SVC11.train(train_data,train_data_Y)\n",
    "my_SVC12 = Support_Vector_Classifier(7500, 0.0000001, 500, 1000)\n",
    "my_SVC12.train(train_data,train_data_Y)\n",
    "my_SVC13 = Support_Vector_Classifier(10000, 0.0000003, 500, 1000)\n",
    "my_SVC13.train(train_data,train_data_Y)\n",
    "my_SVC14 = Support_Vector_Classifier(10000, 0.00000007, 500, 1000)\n",
    "my_SVC14.train(train_data,train_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model hyperparameters:\n",
      "Regularization strength: 10000\n",
      "Learning rate: 1e-07\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  94.110337972167\n",
      "Confusion Matrix:\n",
      " [[1966   96]\n",
      " [ 141 1821]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 1000\n",
      "Learning rate: 1e-07\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  93.61332007952286\n",
      "Confusion Matrix:\n",
      " [[1937   87]\n",
      " [ 170 1830]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 100000\n",
      "Learning rate: 1e-07\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  94.06063618290258\n",
      "Confusion Matrix:\n",
      " [[1966   98]\n",
      " [ 141 1819]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 10000\n",
      "Learning rate: 1e-06\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  93.98608349900596\n",
      "Confusion Matrix:\n",
      " [[1960   95]\n",
      " [ 147 1822]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 10000\n",
      "Learning rate: 1e-08\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  93.88667992047714\n",
      "Confusion Matrix:\n",
      " [[1953   92]\n",
      " [ 154 1825]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 100000\n",
      "Learning rate: 1e-08\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  94.110337972167\n",
      "Confusion Matrix:\n",
      " [[1959   89]\n",
      " [ 148 1828]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 1000\n",
      "Learning rate: 1e-06\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  93.58846918489066\n",
      "Confusion Matrix:\n",
      " [[1928   79]\n",
      " [ 179 1838]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 100000\n",
      "Learning rate: 1e-06\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  92.51988071570577\n",
      "Confusion Matrix:\n",
      " [[1878   72]\n",
      " [ 229 1845]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 1000\n",
      "Learning rate: 1e-08\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  92.71868787276343\n",
      "Confusion Matrix:\n",
      " [[1914  100]\n",
      " [ 193 1817]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 50000\n",
      "Learning rate: 1e-08\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  93.96123260437376\n",
      "Confusion Matrix:\n",
      " [[1963   99]\n",
      " [ 144 1818]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 5000\n",
      "Learning rate: 1e-06\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  93.78727634194831\n",
      "Confusion Matrix:\n",
      " [[1951   94]\n",
      " [ 156 1823]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 12500\n",
      "Learning rate: 1e-07\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  94.06063618290258\n",
      "Confusion Matrix:\n",
      " [[1960   92]\n",
      " [ 147 1825]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 7500\n",
      "Learning rate: 1e-07\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  94.110337972167\n",
      "Confusion Matrix:\n",
      " [[1962   92]\n",
      " [ 145 1825]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 10000\n",
      "Learning rate: 3e-07\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  94.01093439363817\n",
      "Confusion Matrix:\n",
      " [[1957   91]\n",
      " [ 150 1826]] \n",
      "\n",
      "Current model hyperparameters:\n",
      "Regularization strength: 10000\n",
      "Learning rate: 7e-08\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  94.110337972167\n",
      "Confusion Matrix:\n",
      " [[1965   95]\n",
      " [ 142 1822]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print accuracy of all models\n",
    "printAccuracy(my_SVC0, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC1, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC2, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC3, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC4, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC5, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC6, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC7, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC8, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC9, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC10, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC11, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC12, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC13, valid_data, valid_data_Y)\n",
    "printAccuracy(my_SVC14, valid_data, valid_data_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen the hyper-parameters with the following values has performed the best on the validation data:\n",
    "\n",
    "Regularization strength: 10000\n",
    "\n",
    "Learning rate: 1e-07\n",
    "\n",
    "Termination constant: 500\n",
    "\n",
    "Max number of iterations for training: 1000\n",
    "\n",
    "\n",
    "The model with these settings is then used with the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model hyperparameters:\n",
      "Regularization strength: 10000\n",
      "Learning rate: 1e-07\n",
      "Termination constant: 500\n",
      "Max number of iterations for training: 1000\n",
      "Accuracy:  92.94234592445328\n",
      "Confusion Matrix:\n",
      " [[928  61]\n",
      " [ 81 942]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print accuracy of model on testing data with best performing hyper-parameters\n",
    "printAccuracy(my_SVC0, test_data, test_data_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
